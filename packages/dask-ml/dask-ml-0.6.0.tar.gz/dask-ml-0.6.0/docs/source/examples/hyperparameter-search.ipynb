{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Search\n",
    "\n",
    "Most scikit-learn estimators have a set of *hyper-parameters*.\n",
    "These are parameters that are not learned during estimation; they must\n",
    "be set ahead of time.\n",
    "\n",
    "The [`dask-searchcv`](http://dask-searchcv.readthedocs.io/en/latest/) is able to parallelize scikit-learn's hyper-parameter search classes cleverly.\n",
    "It's able to schedule computation using any of dask's schedulers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy import stats\n",
    "\n",
    "from distributed import Client\n",
    "import distributed.joblib\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from dask_searchcv import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import model_selection as ms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is based off [this scikit-learn example](http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html#sphx-glr-auto-examples-model-selection-randomized-search-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get some data\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit a `LogisticRegression`, and compare the `GridSearchCV` and `RandomizedSearchCV` implementations from `scikit-learn` and `dask-searchcv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Grid-search is the brute-force method of hyper-parameter optimization. It fits each combination of parameters, which can be time consuming if you have many hyper-parameters or if you have a fine grid.\n",
    "\n",
    "To use grid search from scikit-learn, you create a dictionary mapping parameter names to lists of values to try.\n",
    "That `param_grid` is passed to `GridSearchCV` along with a classifier (`LogisticRegression` in this example). Notice that `dask_searchcv.GridSearchCV` is a drop-in replacement for `sklearn.model_selection.GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a full grid over all parameters\n",
    "param_grid = {\n",
    "    \"C\": [1e-5, 1e-3, 1e-1, 1],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"penalty\": [\"l1\", \"l2\"]\n",
    "}\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# run grid search\n",
    "dk_grid_search = GridSearchCV(clf, param_grid=param_grid, n_jobs=-1)\n",
    "sk_grid_search = ms.GridSearchCV(clf, param_grid=param_grid, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` objects are fit just like regular estimators: `.fit(X, y)`.\n",
    "\n",
    "First, we'll fit the scikit-learn version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 2.93 seconds for 16 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "sk_grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(sk_grid_search.cv_results_['params'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the `dask-searchcv` version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 1.85 seconds for 16 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "dk_grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(dk_grid_search.cv_results_['params'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search\n",
    "\n",
    "Randomized search is similar in spirit to grid search, but the method of choosing parameters to evaluate differs.\n",
    "With grid search, you specify the parameters to try, and scikit-learn tries each possible combination.\n",
    "Randomized search, on the other hand, takes some *distributions to sample from* and a maximum number of iterations to try. This lets you focus your search on areas where the parameters should perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"C\": stats.beta(1, 3),\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"penalty\": [\"l1\", \"l2\"]\n",
    "}\n",
    "n_iter_search = 100\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scikit-learn\n",
    "sk_random_search = ms.RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                         n_iter=n_iter_search, n_jobs=-1)\n",
    "\n",
    "# dask\n",
    "dk_random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                      n_iter=n_iter_search, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 7.64 seconds for 100 candidates parameter settings.\n"
     ]
    }
   ],
   "source": [
    "# run randomized search\n",
    "start = time()\n",
    "sk_random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 17.11 seconds for 100 candidates parameter settings.\n"
     ]
    }
   ],
   "source": [
    "dk_random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid Repeated Work\n",
    "\n",
    "dask works by building a *task graph* of computations on data. It's able to cache intermediate computations\n",
    "in the graph, to avoid unnescessarily computing something multiple times. This speeds up computations on\n",
    "scikit-learn `Pipeline`s, since the early stages of a pipeline are used for each parameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "\n",
    "grid = {'vect__ngram_range': [(1, 1)],\n",
    "        'tfidf__norm': ['l1', 'l2'],\n",
    "        'clf__alpha': [1e-5, 1e-4, 1e-3, 1e-1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a regular `sklearn.model_selection.GridSearchCV`, we would need to evaluate the `CountVectorizor(ngram_range=(1, 1))` 8 times (once for each of the `tfidf__norm` and `clf__alpha` combintions.\n",
    "\n",
    "With dask, we need only compute it once and the intermediate result is cached and reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sk_grid_search = ms.GridSearchCV(pipeline, grid, n_jobs=-1)\n",
    "dk_grid_search = GridSearchCV(pipeline, grid, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 34.44 seconds for 8 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "dk_grid_search.fit(data.data, data.target)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(dk_grid_search.cv_results_['params'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 40.32 seconds for 8 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "sk_grid_search.fit(data.data, data.target)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(sk_grid_search.cv_results_['params'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
